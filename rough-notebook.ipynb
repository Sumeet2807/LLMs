{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7047ee8-eb34-44c1-a53d-31c2f3bf4fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##  /apps/pytorch/2.0.1/bin/python\n",
    "## /orange/h.azad/s.saini/.env/bin/activate\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "from model import ModelArgs, Transformer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tokenizer import Tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import subprocess as sp\n",
    "import os\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d11f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sents_gen(corpus_filename,bsz=32,max_sent_len=10):\n",
    "    with open(corpus_filename) as f:\n",
    "        lines = f.readlines()\n",
    "    text = ' '.join(lines)\n",
    "    len_text = len(text)\n",
    "    res = re.finditer(r\"\\s\", text)\n",
    "    word_indices = []\n",
    "    for obj in list(res):\n",
    "        word_indices.append(obj.span()[1])\n",
    "    word_indices = np.array(word_indices)\n",
    "    len_indices = len(word_indices)\n",
    "    shuffled_indices = np.arange(0,len_indices)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    \n",
    "    for i in range(0,len_indices,bsz):\n",
    "        j = min(i+bsz,len_indices)\n",
    "        start_indices = shuffled_indices[i:j]\n",
    "        batch = []\n",
    "        for index in start_indices:\n",
    "            batch.append(text[word_indices[index]:word_indices[min(index+max_sent_len,len_indices-1)]])\n",
    "        \n",
    "        yield batch\n",
    "\n",
    "\n",
    "def get_sents_gen_dir(directory,bsz=32,max_sent_len=10):\n",
    "    files = os.listdir(directory)\n",
    "    num_files = len(files)\n",
    "    f_indices = list(range(num_files))\n",
    "    random.shuffle(f_indices)\n",
    "    for i in range(0,num_files,3):\n",
    "        indices=f_indices[i:min(i+3,num_files)]\n",
    "        text = ''\n",
    "        for j in indices:\n",
    "            df = pd.read_parquet(os.path.join(directory,files[f_indices[j]]))\n",
    "            text = '\\n\\n'.join(df['text'].to_list())\n",
    "\n",
    "        \n",
    "        len_text = len(text)\n",
    "        res = re.finditer(r\"\\s\", text)\n",
    "        word_indices = []\n",
    "        for obj in list(res):\n",
    "            word_indices.append(obj.span()[1])\n",
    "        word_indices = np.array(word_indices)\n",
    "        len_indices = len(word_indices)\n",
    "        shuffled_indices = np.arange(0,len_indices)\n",
    "        np.random.shuffle(shuffled_indices)\n",
    "        \n",
    "        \n",
    "        for i in range(0,len_indices,bsz):\n",
    "            j = min(i+bsz,len_indices)\n",
    "            start_indices = shuffled_indices[i:j]\n",
    "            batch = []\n",
    "            for index in start_indices:\n",
    "                batch.append(text[word_indices[index]:word_indices[min(index+max_sent_len,len_indices-1)]])\n",
    "            \n",
    "            yield batch\n",
    "            \n",
    "def get_sents_from_parquets(directory,bsz=10,max_sent_len=10):\n",
    "\n",
    "    all_indices = []\n",
    "    dfs = []\n",
    "\n",
    "    print('Preparing data.....')\n",
    "    \n",
    "    for filename in ['data/0.parquet','data/1.parquet','data/2.parquet','data/3.parquet']:\n",
    "        dfs.append(pd.read_parquet(filename))\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    len_df = len(df)\n",
    "    indices = list(range(len_df))\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(0,len_df,bsz):\n",
    "        yield (df['text'].iloc[indices[i:i+bsz]]).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e719b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import sample_top_p\n",
    "\n",
    "\n",
    "prompts = ['Max is a good dog. He protects kids.', 'John was hungry. He wanted to eat']\n",
    "\n",
    "\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class LanguageModel():\n",
    "    \n",
    "    def __init__(self,model_args,tokenizer,max_seq_len=10,bsz=1,verbose=True,device='cuda',device_index=5):\n",
    "        model_args.vocab_size = tokenizer.n_words\n",
    "        model_args.max_seq_len = max_seq_len\n",
    "        model_args.max_batch_size = bsz\n",
    "        self.bsz = bsz\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.model = Transformer(model_args)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.device_index = device_index\n",
    "        if verbose:\n",
    "            summary(self.model,torch.ones((1,10),dtype=torch.long).to(device))\n",
    "\n",
    "    def compute_loss(self,batch):\n",
    "\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        sent_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in batch]\n",
    "        tokens = torch.full((len(batch), self.max_seq_len), pad_id, dtype=torch.long, device=self.device)\n",
    "        for k, t in enumerate(sent_tokens):\n",
    "            tokens[k, : min(self.max_seq_len,len(t))] = torch.tensor(t[:min(self.max_seq_len,len(t))], dtype=torch.long, device=self.device)\n",
    "        X = tokens[:,:-1]\n",
    "        y = tokens[:,1:]\n",
    "        input_text_mask = (X != pad_id).to(self.device)\n",
    "        output = torch.zeros((X.shape[0],X.shape[1],self.tokenizer.n_words),device=self.device)\n",
    "        output = self.model(X)\n",
    "        output = output*(input_text_mask.int()[...,None])\n",
    "        loss = self.loss_fn(torch.transpose(output,1,-1),y)\n",
    "        return loss\n",
    "    \n",
    "    def train(self,train_corpus,test_corpus=None,epochs=20):\n",
    "        \n",
    "        self.model.train()\n",
    "        writer = SummaryWriter()\n",
    "        \n",
    "        \n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, betas=(0.9, 0.95), eps=1e-05, weight_decay=0.1)\n",
    "        \n",
    "        cnt = 0\n",
    "        for i in range(1,epochs+1):\n",
    "            print('total epochs - ' + str(i))\n",
    "            # sent_gen = get_sents_gen_dir(train_corpus,self.bsz,self.max_seq_len)\n",
    "            sent_gen = get_sents_from_parquets(train_corpus,self.bsz,self.max_seq_len)\n",
    "            \n",
    "            for j, batch in enumerate(sent_gen):\n",
    "                loss = self.compute_loss(batch)\n",
    "                loss.backward()#retain_graph=True)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss_val = loss.detach()\n",
    "                cnt+=1\n",
    "                if not j%100:\n",
    "                    with torch.no_grad():\n",
    "                        if test_corpus is not None:\n",
    "                            test_sent_gen = get_sents_gen(test_corpus,self.bsz,self.max_seq_len)\n",
    "                            losses = []\n",
    "                            for j, batch in enumerate(sent_gen): \n",
    "                                losses.append(self.compute_loss(batch))\n",
    "                            val_loss = torch.mean(losses)\n",
    "                            writer.add_scalar(\"Loss/test\", val_loss, cnt)\n",
    "                        print(loss_val)                    \n",
    "                        writer.add_scalar(\"Mem/train\", get_gpu_memory()[self.device_index], cnt)\n",
    "                        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "                        gens = self.tokenizer.decode(self.generate(prompt_tokens,50)[0])\n",
    "                        for k,gen in enumerate(gens):\n",
    "                            writer.add_text(\"Gen/train\",prompts[k]+'#####'+gen,cnt)\n",
    "                    \n",
    "                # del loss\n",
    "                writer.add_scalar(\"Loss/train\", loss_val, cnt)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens,\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate text sequences based on provided prompts using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n",
    "            max_gen_len (int): Maximum length of the generated text sequence.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n",
    "\n",
    "        Note:\n",
    "            This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "\n",
    "        \"\"\"\n",
    "        print('generating')\n",
    "        params = self.model.params\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos, use_att_cache= True)\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                next_token == self.tokenizer.eos_id\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            if self.tokenizer.eos_id in toks:\n",
    "                eos_idx = toks.index(self.tokenizer.eos_id)\n",
    "                toks = toks[:eos_idx]\n",
    "                probs = probs[:eos_idx] if logprobs else None\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27cc43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[81046, 81046, 81046, 66259, 81046, 81046, 81046, 81046]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec148bb-acee-44b7-b091-cbee49ed9f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Embedding: 1-1                         [-1, 10, 512]             14,336,000\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─TransformerBlock: 2-1             [-1, 10, 512]             --\n",
      "|    |    └─RMSNorm: 3-1                 [-1, 10, 512]             512\n",
      "|    |    └─RMSNorm: 3-2                 [-1, 10, 512]             512\n",
      "|    └─TransformerBlock: 2-2             [-1, 10, 512]             --\n",
      "|    |    └─RMSNorm: 3-3                 [-1, 10, 512]             512\n",
      "|    |    └─RMSNorm: 3-4                 [-1, 10, 512]             512\n",
      "|    └─TransformerBlock: 2-3             [-1, 10, 512]             --\n",
      "|    |    └─RMSNorm: 3-5                 [-1, 10, 512]             512\n",
      "|    |    └─RMSNorm: 3-6                 [-1, 10, 512]             512\n",
      "|    └─TransformerBlock: 2-4             [-1, 10, 512]             --\n",
      "|    |    └─RMSNorm: 3-7                 [-1, 10, 512]             512\n",
      "|    |    └─RMSNorm: 3-8                 [-1, 10, 512]             512\n",
      "|    └─TransformerBlock: 2-5             [-1, 10, 512]             --\n",
      "|    |    └─RMSNorm: 3-9                 [-1, 10, 512]             512\n",
      "|    |    └─RMSNorm: 3-10                [-1, 10, 512]             512\n",
      "|    └─TransformerBlock: 2-6             [-1, 10, 512]             --\n",
      "|    |    └─RMSNorm: 3-11                [-1, 10, 512]             512\n",
      "|    |    └─RMSNorm: 3-12                [-1, 10, 512]             512\n",
      "|    └─TransformerBlock: 2-7             [-1, 10, 512]             --\n",
      "|    |    └─RMSNorm: 3-13                [-1, 10, 512]             512\n",
      "|    |    └─RMSNorm: 3-14                [-1, 10, 512]             512\n",
      "|    └─TransformerBlock: 2-8             [-1, 10, 512]             --\n",
      "|    |    └─RMSNorm: 3-15                [-1, 10, 512]             512\n",
      "|    |    └─RMSNorm: 3-16                [-1, 10, 512]             512\n",
      "├─RMSNorm: 1-2                           [-1, 10, 512]             512\n",
      "├─Linear: 1-3                            [-1, 10, 28000]           14,336,000\n",
      "==========================================================================================\n",
      "Total params: 28,680,704\n",
      "Trainable params: 28,680,704\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 79.28\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 6.08\n",
      "Params size (MB): 109.41\n",
      "Estimated Total Size (MB): 115.49\n",
      "==========================================================================================\n",
      "total epochs - 1\n",
      "Preparing data.....\n",
      "tensor(10.3731, device='cuda:0')\n",
      "generating\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer(\u001b[39m'\u001b[39m\u001b[39mdata/vocab/tinystories28000.model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m lm \u001b[39m=\u001b[39m LanguageModel(model_args,tokenizer,bsz\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,max_seq_len\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,device_index\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m lm\u001b[39m.\u001b[39;49mtrain(\u001b[39m'\u001b[39;49m\u001b[39mdata/tinystories_corpus/\u001b[39;49m\u001b[39m'\u001b[39;49m,epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m \u001b[39m# LM_train(model_args,tokenizer,'data/alice_in_wonderland.txt',max_sent_len=128,bsz=32,epochs=1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 66\u001b[0m, in \u001b[0;36mLanguageModel.train\u001b[0;34m(self, train_corpus, test_corpus, epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m j, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sent_gen):\n\u001b[1;32m     65\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(batch)\n\u001b[0;32m---> 66\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\u001b[39m#retain_graph=True)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/projects/llama/.env/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/projects/llama/.env/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_args = ModelArgs()\n",
    "model_args.dim= 512\n",
    "model_args.n_layers= 8\n",
    "model_args.n_heads= 8\n",
    "model_args.n_kv_heads = None\n",
    "model_args.vocab_size= None  # defined later by tokenizer\n",
    "model_args.multiple_of = 32  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "model_args.ffn_dim_multiplier = None\n",
    "model_args.norm_eps= 1e-5\n",
    "model_args.max_batch_size= None\n",
    "model_args.max_seq_len= None\n",
    "\n",
    "device = 'cuda'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "tokenizer = Tokenizer('data/vocab/tinystories28000.model')\n",
    "\n",
    "lm = LanguageModel(model_args,tokenizer,bsz=128,max_seq_len=200,device_index=5)\n",
    "lm.train('data/tinystories_corpus/',epochs=20)\n",
    "\n",
    "# LM_train(model_args,tokenizer,'data/alice_in_wonderland.txt',max_sent_len=128,bsz=32,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a372da8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating\n",
      "John wanted to run, so he started running. Maria wanted to dance, so she started dancing. Julia wanted to jump, so she#####ran after him. John was so fast that he was almost dizzy. He couldn't move. He was so dizzy that he almost fell over. Alice was very worried. She decided to try again. She took a deep breath and jumped. She\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = ['John wanted to run, so he started running. Maria wanted to dance, so she started dancing. Julia wanted to jump, so she']\n",
    "prompt_tokens = [tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "gens = tokenizer.decode(lm.generate(prompt_tokens,50)[0])\n",
    "for k,gen in enumerate(gens):\n",
    "    print(prompts[k]+'#####'+gen+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53df75-2143-4339-b030-85cc1f1d5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd4c53-67fb-41af-a2d6-8a544dcce617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='data/alice_in_wonderland.txt', model_prefix='alice', vocab_size=2642, user_defined_symbols=['foo', 'bar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842f366-2b52-4a4b-9e41-6b9ae91829b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re \n",
    "s = 'sdfds dsf sdf sdsdfs d fsf df'\n",
    "print(re.findall(r\"\\s\", s))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe313262-bf7f-4f33-90f3-804eb3d73682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random \n",
    "import numpy as np\n",
    "# initializing string\n",
    "# test_str = 'sdfds dsf sdf sdsdfs d fsf df'\n",
    " \n",
    "# # Using regex\n",
    "# # Check for spaces\n",
    "# res = re.finditer(r\"\\s\", test_str)\n",
    "# word_indices = []\n",
    "# for obj in list(res):\n",
    "#     word_indices.append(obj.span()[1])\n",
    "    \n",
    "# shuffled_indices = word_indices.copy()\n",
    "# random.shuffle(shuffled_indices)\n",
    "\n",
    "# for \n",
    "    \n",
    "def get_sents_gen(corpus_filename,bsz=32,max_sent_len=64):\n",
    "    with open(corpus_filename) as f:\n",
    "        lines = f.readlines()\n",
    "    text = ' '.join(lines)\n",
    "    len_text = len(text)\n",
    "    res = re.finditer(r\"\\s\", text)\n",
    "    word_indices = []\n",
    "    for obj in list(res):\n",
    "        word_indices.append(obj.span()[1])\n",
    "    word_indices = np.array(word_indices)\n",
    "    len_indices = len(word_indices)\n",
    "    shuffled_indices = np.arange(0,len_indices)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    \n",
    "    for i in range(0,len_indices,bsz):\n",
    "        j = min(i+bsz,len_indices)\n",
    "        start_indices = shuffled_indices[i:j]\n",
    "        batch = []\n",
    "        for index in start_indices:\n",
    "            batch.append(text[word_indices[index]:word_indices[min(index+max_sent_len,len_indices)]])\n",
    "        \n",
    "        yield batch\n",
    "        \n",
    "\n",
    "dgen = get_sents_gen('data/alice_in_wonderland.txt')\n",
    "\n",
    "for batch in dgen:\n",
    "    print(batch)\n",
    "    break\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973132e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_gen():\n",
    "    for s in ['sad fa sff fdsf','fsfs sf d','sdsdasdf fsdgdf sf']:\n",
    "        yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad157d5a-55ef-4f2d-991f-1ba53c8579f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sg = sent_gen()\n",
    "spm.SentencePieceTrainer.train(input=sg, model_prefix='test/test', vocab_size=10,pad_id=0, bos_id=1,eos_id=2,unk_id=3)\n",
    "sp = spm.SentencePieceProcessor(model_file='test/test.model')\n",
    "sp.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a462a-1e95-4dc1-a1d4-65332b8c8948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp.pad_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82ff59-4a03-4e3a-8050-97194fb741f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp.decode([770, 537, 138, 22, 58, 11, 404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d11be8-78c5-4e28-8d98-de2f387ed240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "cache_mask_list = []\n",
    "xk_extended_list = []\n",
    "\n",
    "start_pos = 5\n",
    "seqlen = 3\n",
    "max_seq_len = 10\n",
    "bsz = 4\n",
    "xk = 3*torch.ones((bsz,seqlen,1,1))\n",
    "cache_k = torch.zeros((bsz,max_seq_len,1,1))\n",
    "\n",
    "\n",
    "if start_pos > 0:\n",
    "    cache_mask_list.append(torch.ones((cache_k.shape[0],start_pos,cache_k.shape[2],cache_k.shape[3])))\n",
    "    xk_extended_list.append(torch.zeros((cache_k.shape[0],start_pos,cache_k.shape[2],cache_k.shape[3])))\n",
    "    \n",
    "cache_mask_list.append(torch.zeros(xk.shape))\n",
    "xk_extended_list.append(xk)\n",
    "\n",
    "if (start_pos + seqlen) < cache_k.shape[1]:\n",
    "    cache_mask_list.append(torch.ones((cache_k.shape[0],cache_k.shape[1] - (start_pos + seqlen),cache_k.shape[2],cache_k.shape[3])))\n",
    "    xk_extended_list.append(torch.zeros((cache_k.shape[0],cache_k.shape[1] - (start_pos + seqlen),cache_k.shape[2],cache_k.shape[3])))\n",
    "    \n",
    "cache_mask = torch.cat(cache_mask_list,dim=1)\n",
    "xk_extended = torch.cat(xk_extended_list,dim=1)\n",
    "\n",
    "print(cache_mask.shape,cache_k.shape,xk_extended.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732067a-5f69-403a-8af5-3487174f23b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cache_mask,xk_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c17048-627a-4d55-8e3c-d39888e434f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:04:59) [GCC 10.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d49a3689aba7aac1371e9c0bf51100098054447b8ccd65c7c0ac4105b204a3be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
