{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  /apps/pytorch/2.0.1/bin/python\n",
    "## /orange/h.azad/s.saini/.env/bin/activate\n",
    "\n",
    "\n",
    "\n",
    "from slm.models.llama import ModelArgs\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from slm.models.lm import LanguageModel\n",
    "from slm.data.dataparquet import ParquetDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArgs()\n",
    "model_args.dim= 512\n",
    "model_args.n_layers= 8\n",
    "model_args.n_heads= 8\n",
    "model_args.n_kv_heads = None\n",
    "model_args.vocab_size= None  # defined later by tokenizer\n",
    "model_args.multiple_of = 32  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "model_args.ffn_dim_multiplier = None\n",
    "model_args.norm_eps= 1e-5\n",
    "model_args.max_batch_size= None\n",
    "model_args.max_seq_len= None\n",
    "\n",
    "device = 'cuda'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# tokenizer = Tokenizer('data/vocab/tinystories28000.model')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "lm = LanguageModel(model_args,tokenizer,bsz=32,max_seq_len=1024,device_index=5)\n",
    "dset = ParquetDataset(['data/0.parquet'])\n",
    "train_loader = DataLoader(dset, batch_size=32, shuffle=True)\n",
    "lm.train(train_loader,epochs=20)\n",
    "\n",
    "# LM_train(model_args,tokenizer,'data/alice_in_wonderland.txt',max_sent_len=128,bsz=32,epochs=1)\n",
    "\n",
    "# model_engine, optimizer, _, _ = deepspeed.initialize(model=lm.model,\n",
    "#                                                      model_parameters=lm.model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n",
    "    model.train()\n",
    "    ddp_loss = torch.zeros(2).to(rank)\n",
    "    if sampler:\n",
    "        sampler.set_epoch(epoch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(rank), target.to(rank)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target, reduction='sum')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ddp_loss[0] += loss.item()\n",
    "        ddp_loss[1] += len(data)\n",
    "\n",
    "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
    "    if rank == 0:\n",
    "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp.wrap import (\n",
    "    size_based_auto_wrap_policy,\n",
    "    enable_wrap,\n",
    "    wrap,\n",
    ")\n",
    "import functools\n",
    "\n",
    "\n",
    "def fsdp_main(rank, world_size, args):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "\n",
    "    dset = ParquetDataset(['data/0.parquet'])\n",
    "    train_loader = DataLoader(dset, batch_size=32, shuffle=True)\n",
    "    sampler = DistributedSampler(dset, rank=rank, num_replicas=world_size, shuffle=True)\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler}\n",
    "    cuda_kwargs = {'num_workers': 2,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': False}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dset,**train_kwargs)\n",
    "    # my_auto_wrap_policy = functools.partial(\n",
    "    #     size_based_auto_wrap_policy, min_num_params=100\n",
    "    # )\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "\n",
    "    init_start_event = torch.cuda.Event(enable_timing=True)\n",
    "    init_end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    model_args = ModelArgs()\n",
    "    model_args.dim= 512\n",
    "    model_args.n_layers= 8\n",
    "    model_args.n_heads= 8\n",
    "    model_args.n_kv_heads = None\n",
    "    model_args.vocab_size= None  # defined later by tokenizer\n",
    "    model_args.multiple_of = 32  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    model_args.ffn_dim_multiplier = None\n",
    "    model_args.norm_eps= 1e-5\n",
    "    model_args.max_batch_size= None\n",
    "    model_args.max_seq_len= None\n",
    "\n",
    "    device = 'cuda'\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    # tokenizer = Tokenizer('data/vocab/tinystories28000.model')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "    lm = LanguageModel(model_args,tokenizer,bsz=32,max_seq_len=1024,device=rank,device_index=5)\n",
    "    lm.fsdp()\n",
    "    lm.train(train_loader,sampler=sampler,epochs=20)\n",
    "    # optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    # init_start_event.record()\n",
    "    # for epoch in range(1, args.epochs + 1):\n",
    "    #     train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n",
    "    #     test(model, rank, world_size, test_loader)\n",
    "    #     scheduler.step()\n",
    "\n",
    "    init_end_event.record()\n",
    "\n",
    "    # if rank == 0:\n",
    "    #     print(f\"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event) / 1000}sec\")\n",
    "        # print(f\"{model}\")\n",
    "\n",
    "    # if args.save_model:\n",
    "    #     # use a barrier to make sure training is done on all ranks\n",
    "    #     dist.barrier()\n",
    "    #     states = model.state_dict()\n",
    "    #     if rank == 0:\n",
    "    #         torch.save(states, \"mnist_cnn.pt\")\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, files, text_col='text',shuffle=True):\n",
    "\n",
    "        dfs = []\n",
    "        print('Preparing data.....')\n",
    "    \n",
    "        for filename in files:\n",
    "            dfs.append(pd.read_parquet(filename))\n",
    "\n",
    "        df = pd.concat(dfs)\n",
    "        self.data = df['text'].to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = ParquetDataset(['data/0.parquet'])#,'data/1.parquet','data/2.parquet','data/3.parquet'])\n",
    "\n",
    "\n",
    "dloader = DataLoader(dset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sents_from_parquets(directory,bsz=10,max_sent_len=10):\n",
    "\n",
    "    all_indices = []\n",
    "    dfs = []\n",
    "\n",
    "    print('Preparing data.....')\n",
    "    \n",
    "    for filename in ['data/0.parquet','data/1.parquet','data/2.parquet','data/3.parquet']:\n",
    "        dfs.append(pd.read_parquet(filename))\n",
    "\n",
    "    for filename in ['data/dolly/1.parquet','data/squadv2/1.parquet']:\n",
    "        dfs.append(pd.read_parquet(filename,columns=['text']))\n",
    "    \n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    len_df = len(df)\n",
    "    indices = list(range(len_df))\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(0,len_df,bsz):\n",
    "        yield (df['text'].iloc[indices[i:i+bsz]]).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x) is list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:04:59) [GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d49a3689aba7aac1371e9c0bf51100098054447b8ccd65c7c0ac4105b204a3be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
